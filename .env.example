# Clinical Annotation Platform - Environment Configuration

# vLLM Server Configuration
# URL of the vLLM server for LLM inference
# Use host.docker.internal to reach the host machine from inside Docker containers
VLLM_API_URL=http://host.docker.internal:8000

# vLLM Docker image (change if your NVIDIA driver doesn't support the latest CUDA)
# For CUDA 12.4 / driver 550.x, use: vastai/vllm:v0.8.5-cuda-12.4-pytorch-2.6.0-py312
VLLM_IMAGE=vllm/vllm-openai:latest

# vLLM Model Configuration (used by docker-compose.vllm.yml)
VLLM_MODEL=unsloth/medgemma-27b-text-it-unsloth-bnb-4bit
VLLM_MAX_MODEL_LEN=16384
VLLM_MAX_NUM_SEQS=32
VLLM_GPU_MEM_UTIL=0.85
VLLM_HOST_PORT=8080
VLLM_EXTRA_ARGS=
# HF_TOKEN=

# Backend API Configuration
# Port for the annotation API (default: 8001)
ANNOTATION_API_PORT=8001

# Frontend Configuration
# URL for the frontend to connect to the API
NEXT_PUBLIC_API_URL=http://localhost:8001

# Pipeline Configuration
# Path for pipeline results storage
APP_PATH=/data/results/data

# NLP Integration URLs (used by pipeline components)
NLP_BACKEND_URL=http://localhost:8001
NLP_FRONTEND_URL=http://localhost:3000
